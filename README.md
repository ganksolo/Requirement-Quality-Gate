# ReqGate - Requirement Quality Gate

> ðŸš€ AI-powered requirement quality assessment for PRD/Tickets

## Overview

ReqGate is a quality gate system that uses AI to automatically evaluate product requirements before they enter technical review. It catches issues early, provides actionable feedback, and improves the overall quality of requirements.

### Key Features

- **Automatic Structuring**: Convert messy requirement text into standardized PRD format
- **Quality Scoring**: LLM-based evaluation against configurable rubric
- **Hard Gate**: Deterministic pass/reject decisions with blocking issue detection
- **Fallback Mechanism**: Graceful degradation when structuring fails
- **Input Guardrail**: Protection against PII leakage and prompt injection

## Current Status

**Phase 1**: Foundation & Scoring Core âœ… Complete  
**Phase 2**: Structuring & Workflow âœ… Complete

### Phase 2 Features

- âœ… PRD_Draft schema for structured requirements
- âœ… Structuring Agent with anti-hallucination measures
- âœ… Input Guardrail for input validation and security
- âœ… LangGraph workflow orchestration
- âœ… Fallback mechanism for graceful degradation
- âœ… Retry logic with exponential backoff

## Quick Start

### Prerequisites

- Python 3.14+
- [uv](https://github.com/astral-sh/uv) (recommended) or pip

### Installation

```bash
# Clone the repository
git clone <repo-url>
cd Requirement-Quality-Gate

# Create virtual environment and install dependencies
uv venv
source .venv/bin/activate  # or `.venv\Scripts\activate` on Windows
uv pip install -e ".[dev]"

# Copy environment file
cp .env.example .env
# Edit .env and add your OPENROUTER_API_KEY
```

### Running the Server

```bash
# Development mode with auto-reload
uvicorn src.reqgate.app.main:app --reload --port 8000

# Access the API docs at http://localhost:8000/docs
```

### Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src/reqgate --cov-report=html

# Run specific test file
pytest tests/test_workflow_integration.py -v
```

### Code Quality

```bash
# Lint check
ruff check src/ tests/

# Format code
ruff format src/ tests/

# Type check
mypy src/
```

## Project Structure

```
reqgate/
â”œâ”€â”€ src/reqgate/
â”‚   â”œâ”€â”€ app/           # FastAPI application
â”‚   â”œâ”€â”€ api/           # API routes
â”‚   â”œâ”€â”€ config/        # Configuration management
â”‚   â”œâ”€â”€ schemas/       # Pydantic models
â”‚   â”‚   â”œâ”€â”€ inputs.py      # RequirementPacket
â”‚   â”‚   â”œâ”€â”€ outputs.py     # TicketScoreReport
â”‚   â”‚   â”œâ”€â”€ internal.py    # PRD_Draft, AgentState
â”‚   â”‚   â””â”€â”€ config.py      # WorkflowConfig
â”‚   â”œâ”€â”€ agents/        # AI agents
â”‚   â”‚   â”œâ”€â”€ scoring.py     # Scoring Agent
â”‚   â”‚   â””â”€â”€ structuring.py # (deprecated, see workflow/)
â”‚   â”œâ”€â”€ gates/         # Quality gate logic
â”‚   â”‚   â”œâ”€â”€ rules.py       # Rubric Loader
â”‚   â”‚   â””â”€â”€ decision.py    # Hard Gate
â”‚   â”œâ”€â”€ adapters/      # External integrations
â”‚   â”‚   â””â”€â”€ llm.py         # LLM Client with retry
â”‚   â”œâ”€â”€ workflow/      # LangGraph workflows (Phase 2)
â”‚   â”‚   â”œâ”€â”€ graph.py       # Workflow definition
â”‚   â”‚   â”œâ”€â”€ errors.py      # Workflow exceptions
â”‚   â”‚   â””â”€â”€ nodes/         # Workflow nodes
â”‚   â””â”€â”€ observability/ # Logging & monitoring
â”œâ”€â”€ tests/             # Test suite (276+ tests)
â”œâ”€â”€ config/            # Configuration files
â”‚   â”œâ”€â”€ scoring_rubric.yaml
â”‚   â””â”€â”€ guardrail_config.yaml
â”œâ”€â”€ prompts/           # LLM prompt templates
â”‚   â””â”€â”€ structuring_agent_v1.txt
â””â”€â”€ docs/              # Documentation
```

## Workflow

ReqGate processes requirements through a multi-stage pipeline:

```
Input Text
    â†“
[Input Guardrail] â†’ Validate length, detect PII, block injection
    â†“
[Structuring Agent] â†’ Convert to structured PRD_Draft
    â†“ (fallback if fails)
[Scoring Agent] â†’ Evaluate quality against rubric
    â†“
[Hard Gate] â†’ Make pass/reject decision
    â†“
Output
```

### Fallback Mode

If the Structuring Agent fails to process input:
1. Error is logged
2. Scoring Agent uses raw text directly
3. Score penalty (-5 points) is applied
4. Workflow continues without interruption

## API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Health check |

> Note: Full API endpoints will be added in Phase 3.

## Configuration

All configuration is via environment variables. See `.env.example` for available options.

### Key Configuration Options

| Variable | Default | Description |
|----------|---------|-------------|
| `OPENROUTER_API_KEY` | - | Required for LLM calls |
| `LLM_MODEL` | `openai/gpt-4o` | Primary LLM model |
| `ENABLE_STRUCTURING` | `true` | Toggle structuring agent |
| `ENABLE_GUARDRAIL` | `true` | Toggle input guardrail |
| `GUARDRAIL_MODE` | `lenient` | `strict` or `lenient` |
| `MAX_LLM_RETRIES` | `3` | LLM retry attempts |
| `STRUCTURING_TIMEOUT` | `20` | Structuring timeout (seconds) |
| `DEFAULT_THRESHOLD` | `60` | Pass/reject score threshold |

## Documentation

- [Architecture](docs/architecture.md) - System architecture and components
- [Workflow](docs/workflow.md) - LangGraph workflow details
- [Prompts](docs/prompts.md) - Prompt template documentation
- [START_HERE](START_HERE.md) - Project entry point
- [SPEC_GUIDE](SPEC_GUIDE.md) - Spec usage guide

## Development

### Spec-Driven Development

This project follows Spec-Driven Development methodology:

1. Create Spec (`requirements.md`, `design.md`, `tasks.md`)
2. Review and approve Spec
3. Execute tasks from Spec
4. Verify milestone completion

See `.kiro/specs/` for current and completed Specs.

### Core Principles

- **Schema-Driven**: All data flows through Pydantic schemas
- **Type Safety**: All functions have type annotations
- **Test Coverage**: Core modules > 80% coverage
- **Quality Checks**: ruff + mypy must pass

## License

MIT
